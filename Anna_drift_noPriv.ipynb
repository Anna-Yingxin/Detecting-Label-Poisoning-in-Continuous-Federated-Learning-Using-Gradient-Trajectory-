{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IsD4mAzMFXs"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy\n",
        "from scipy.stats import zscore\n",
        "import copy\n",
        "\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam, SGD\n",
        "import torchvision\n",
        "import torchvision.transforms as tt\n",
        "import torchvision.models as models\n",
        "from torchvision.datasets import MNIST, ImageFolder\n",
        "from torchvision.utils import make_grid\n",
        "from torch.utils.data import random_split, DataLoader, Subset,SubsetRandomSampler\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "import time\n",
        "\n",
        "from copy import deepcopy\n",
        "import logging"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "OV5K4fe2Oj-A"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYx8synfN5t0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f985a33b-a43d-4476-dbd1-60dc8a16ca3a"
      },
      "source": [
        "transform = tt.Compose([tt.ToTensor(),\n",
        "                    tt.Normalize((0.1307,), (0.3081,))])\n",
        "\n",
        "train_ds = MNIST(root='.', train=True, download=True, transform=transform)\n",
        "test_ds = MNIST(root='.', train=False, download=True, transform=transform)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 108595868.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 138917080.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 1648877/1648877 [00:00<00:00, 29661314.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 26907526.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_UGSS5VYbHh"
      },
      "source": [
        "batch_size=100\n",
        "\n",
        "train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers = 4, pin_memory=True)\n",
        "test_dl = DataLoader(test_ds, batch_size, num_workers = 4, pin_memory=True)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "caTHV6-MY1ZC",
        "outputId": "8c2a1fd7-5710-493b-9da0-1771feda9945"
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available else 'cpu'\n",
        "device"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ic7FRbWGZR9U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db252c09-0767-4157-fe8f-8b1e344f10b2"
      },
      "source": [
        "class MnistNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Sequential(nn.Conv2d(1, 10, 5),\n",
        "                                  nn.MaxPool2d(2),\n",
        "                                  nn.ReLU())\n",
        "        self.conv2 = nn.Sequential(nn.Conv2d(10, 20, kernel_size=5),\n",
        "                                  nn.Dropout2d(),\n",
        "                                  nn.MaxPool2d(2),\n",
        "                                  nn.ReLU())\n",
        "        self.fc1 = nn.Sequential(nn.Flatten(),\n",
        "                                nn.Linear(320, 50),\n",
        "                                nn.Dropout(),\n",
        "                                nn.ReLU())\n",
        "        self.fc2 = nn.Linear(50, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "print(MnistNet())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MnistNet(\n",
            "  (conv1): Sequential(\n",
            "    (0): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (2): ReLU()\n",
            "  )\n",
            "  (conv2): Sequential(\n",
            "    (0): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (1): Dropout2d(p=0.5, inplace=False)\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (3): ReLU()\n",
            "  )\n",
            "  (fc1): Sequential(\n",
            "    (0): Flatten(start_dim=1, end_dim=-1)\n",
            "    (1): Linear(in_features=320, out_features=50, bias=True)\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): ReLU()\n",
            "  )\n",
            "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heolWQ0tc40Q"
      },
      "source": [
        "def test(model, test_dl, criterion):\n",
        "    with torch.no_grad():\n",
        "        model.to(device)\n",
        "        model.eval()\n",
        "        batch_loss, batch_acc = [], []\n",
        "        for images, labels in test_dl:\n",
        "            if torch.cuda.is_available():\n",
        "              images = images.cuda()\n",
        "              # labels = torch.tensor(labels)\n",
        "              labels = labels.cuda()\n",
        "\n",
        "            logits = model(images)\n",
        "            loss = criterion(logits, labels)\n",
        "            batch_loss.append(loss.cpu())\n",
        "            pred = torch.argmax(logits, dim=1)\n",
        "            batch_acc.append(accuracy_score(labels.cpu(), pred.cpu()))\n",
        "        model.cpu()\n",
        "        return sum(batch_loss)/len(batch_loss), sum(batch_acc)/len(batch_acc)\n",
        "\n",
        "def fit(epochs, model, optimizer, criterion, train_dl, test_dl):\n",
        "    train_loss, train_acc, test_loss, test_acc = [], [], [], []\n",
        "    attack=None\n",
        "    train_attack=[]\n",
        "    for epoch in range(1,epochs+1):\n",
        "        val=random.random()\n",
        "        if val>0.20:\n",
        "          attack=True\n",
        "        else:\n",
        "          attack=False\n",
        "\n",
        "        trainl, traina, _ = train(model, train_dl, optimizer,attack)\n",
        "        testl , testa = test(model, test_dl, criterion)\n",
        "        train_loss.append(trainl.detach().numpy())\n",
        "        train_acc.append(traina)\n",
        "\n",
        "        train_attack.append(attack)\n",
        "\n",
        "        test_loss.append(testl.detach().numpy())\n",
        "        test_acc.append(testa)\n",
        "        print(f'Epoch {epoch} - train_loss : {trainl :.4f}, train_acc : {traina:.4f}, test_loss : {testl:.4f}, test_acc : {testa:0.4f}')\n",
        "\n",
        "    history = {'train_loss' : train_loss,\n",
        "               'train_acc' : train_acc,\n",
        "               'test_loss' : test_loss,\n",
        "               'test_acc' : test_acc,\n",
        "               'train_attacked_history' : train_attack}\n",
        "    return history\n",
        "\n",
        "epochs = 3\n",
        "model = MnistNet()\n",
        "optimizer = Adam(model.parameters(), lr = 0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# baseline_history = fit(epochs, model, optimizer, criterion, train_dl, test_dl)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train with attack"
      ],
      "metadata": {
        "id": "zZn20rnrZJST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "std_dev = 1 ###################### Attack 2 Parameter\n",
        "\n",
        "\"\"\"\n",
        "Get historical gradients\n",
        "\"\"\"\n",
        "def train(model, train_dl, optimizer, ID, attack_type, attack=False, hist_grads=None, beta=0.999):\n",
        "    model.to(device)\n",
        "\n",
        "    # get global weight\n",
        "    global_w = deepcopy(model.state_dict())\n",
        "    global_w = torch.cat([v.flatten() for v in global_w.values()])\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    batch_loss, batch_acc = [], []\n",
        "    hist_w_round = []\n",
        "    for images, labels in train_dl:\n",
        "        '''\n",
        "        Attack 1 type:\n",
        "        Single label flipping: for the selected client, flip the label 7 into label 1\n",
        "        '''\n",
        "        if attack and attack_type == 'A1': ###################### Attack 1\n",
        "            labels = label_poisoning(labels)\n",
        "\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        images = images.float()\n",
        "        logits = model(images)\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        # get local weight and gradient\n",
        "        local_w = deepcopy(model.state_dict())\n",
        "\n",
        "        optimizer.step()\n",
        "        batch_loss.append(loss.cpu())\n",
        "        pred = torch.argmax(logits, dim=1)\n",
        "        batch_acc.append(accuracy_score(labels.cpu(), pred.cpu()))\n",
        "\n",
        "    hist_w_round.append(local_w)\n",
        "\n",
        "    \"\"\"mean of historical gradients\"\"\"\n",
        "    mean_weights = {}\n",
        "    for key in hist_w_round[0]:\n",
        "        param_stack = torch.stack([w[key] for w in hist_w_round])\n",
        "        mean_weights[key] = torch.mean(param_stack, dim=0)\n",
        "    local_w_mean = torch.cat([v.flatten() for v in mean_weights.values()])\n",
        "    local_w = torch.cat([v.flatten() for v in local_w.values()])\n",
        "\n",
        "    model.cpu()\n",
        "\n",
        "    return sum(batch_loss)/len(batch_loss), sum(batch_acc)/len(batch_acc), local_w_mean, local_w\n",
        "\n",
        "# flip all with 7 into 1\n",
        "def label_poisoning(label):\n",
        "    source_label=7\n",
        "    target_label=1\n",
        "    label[label == source_label] = target_label\n",
        "    return label"
      ],
      "metadata": {
        "id": "O4aVPAATZOEE"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Njdb15hClCfY"
      },
      "source": [
        "# Train Clients\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Method 0\n",
        "'''\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "def find_attacker_id(clf):\n",
        "    count_1 = sum(clf.labels_ == 1)\n",
        "    count_0 = sum(clf.labels_ == 0)\n",
        "    mal_label = 0 if count_1 > count_0 else 1\n",
        "    atk_id = np.where(clf.labels_ == mal_label)[0]\n",
        "    atk_id = set(atk_id.reshape((-1)))\n",
        "    return atk_id\n",
        "\n",
        "def find_targeted_attack(dict_hist_grad):\n",
        "    value_hist_grad = np.array([v.cpu().numpy() for v in dict_hist_grad.values()])\n",
        "    id_hist_grad = np.array(list(dict_hist_grad.keys()))\n",
        "\n",
        "    cluster = KMeans(n_clusters=2, random_state=0).fit(value_hist_grad)\n",
        "\n",
        "    attacker = find_attacker_id(cluster)\n",
        "    attacker_id = id_hist_grad[list(attacker)]\n",
        "\n",
        "    logging.info(f\"This round TARGETED ATTACK: {attacker_id}\")\n",
        "\n",
        "    return attacker_id"
      ],
      "metadata": {
        "id": "mEW4hE4L3DSn"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsJXESEctevc"
      },
      "source": [
        "def train_clients(client_models, client_optimizers, server_model, criterion, client_dls, num, attack=False):\n",
        "    client_loss, client_acc = [], []\n",
        "    client_hist_grads_dicts = [{} for _ in client_models]\n",
        "    client_curr_grads_dicts = [{} for _ in client_models]\n",
        "    attacked_clients = []\n",
        "\n",
        "    for i, (model, optimizer, train_dl) in enumerate(zip(client_models, client_optimizers, client_dls)):\n",
        "        model.load_state_dict(server_model.state_dict())\n",
        "        hist_grads = client_hist_grads_dicts[i]\n",
        "\n",
        "        ###################### Select Attack\n",
        "        is_attacked = False\n",
        "        attack_type = '0'\n",
        "        if num<=40:\n",
        "            if (i in [0,1,2,3,4,5,6,7,8]):\n",
        "                is_attacked = True\n",
        "                attack_type = 'A1'\n",
        "\n",
        "        attacked_clients.append(is_attacked)\n",
        "        closs, cacc, hist_grads, curr_grads = train(model, train_dl, optimizer, i, attack_type, is_attacked, hist_grads=hist_grads)\n",
        "\n",
        "        client_loss.append(closs)\n",
        "        client_acc.append(cacc)\n",
        "        client_hist_grads_dicts[i] = hist_grads\n",
        "        client_curr_grads_dicts[i] = curr_grads\n",
        "\n",
        "    client_hist_grads_formatted = {client_id: hist_grads for client_id, hist_grads in enumerate(client_hist_grads_dicts)}\n",
        "    client_curr_grads_formatted = {client_id: curr_grads for client_id, curr_grads in enumerate(client_curr_grads_dicts)}\n",
        "\n",
        "    return sum(client_loss)/len(client_loss), sum(client_acc)/len(client_acc), \\\n",
        "            client_hist_grads_formatted, client_curr_grads_formatted, attacked_clients\n",
        "\n",
        "\n",
        "def fedavg(client_models, server_model, historical_grads, current_grads, loss_difference_1, loss_difference_2):\n",
        "    server_new_dict = {}\n",
        "\n",
        "    n = len(client_models)\n",
        "    server_new_dict = {}\n",
        "    for model in client_models:\n",
        "        client_dict =  model.state_dict()\n",
        "        for name in client_dict:\n",
        "            server_new_dict[name] = server_new_dict.get(name, 0) + client_dict[name]\n",
        "    server_new_dict = {k : v/n for k, v in server_new_dict.items()}\n",
        "    server_model.load_state_dict(server_new_dict)\n",
        "\n",
        "    \"\"\"\n",
        "    Perform Cluster on historical_grads & current_grads for comparison:\n",
        "    Only perform cluster when the main model converges\n",
        "    \"\"\"\n",
        "    combined_results = 'No Cluster Performed'\n",
        "    if loss_difference_1 <= 1 and loss_difference_2 <= 1:\n",
        "        cluster_results_1 = find_targeted_attack(historical_grads)\n",
        "        print(\"cluster results of historical\", cluster_results_1)\n",
        "\n",
        "        cluster_results_2 = find_targeted_attack(current_grads)\n",
        "        print(\"cluster results of current\", cluster_results_2)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Simulate Distribution Drift\n",
        "\"\"\"\n",
        "drift_round = 20\n",
        "\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import random\n",
        "\n",
        "def adjust_dataset(dataset, label_reduction_map):\n",
        "    indices = []\n",
        "    for idx, (data, label) in enumerate(dataset):\n",
        "        factor = label_reduction_map.get(label, 1)  # Use label directly\n",
        "        if random.random() < factor:\n",
        "            indices.append(idx)\n",
        "    return Subset(dataset, indices)\n",
        "\n",
        "\n",
        "def create_data_loaders(train_ds, n, label_reduction_map):\n",
        "    train_ds = adjust_dataset(train_ds, label_reduction_map)\n",
        "\n",
        "    size = len(train_ds) // n\n",
        "    last_size = size + len(train_ds) % n\n",
        "\n",
        "    client_ds = random_split(train_ds, [size]*(n-1) + [last_size])\n",
        "    client_dls = [DataLoader(ds, batch_size, shuffle=True, num_workers=4, pin_memory=True)for ds in client_ds]\n",
        "\n",
        "    client_models = [MnistNet() for _ in range(n)]\n",
        "    client_optimizers = [Adam(model.parameters(), 0.001) for model in client_models]\n",
        "\n",
        "    return client_dls, client_models, client_optimizers\n",
        "\n",
        "def iid_clients(train_ds, n, epoch):\n",
        "    client_dls_list = []\n",
        "    for round_num in range(epoch):\n",
        "        if round_num % drift_round == 0:\n",
        "            label_reduction_map = {2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1}  # Reduction factors for labels 2-5\n",
        "\n",
        "            client_dls, client_models, client_optimizers = create_data_loaders(train_ds, n, label_reduction_map)\n",
        "            client_dls_list.append(client_dls)\n",
        "\n",
        "    return client_dls_list, client_models, client_optimizers"
      ],
      "metadata": {
        "id": "masDNDkOyl_U"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_fedavg(epochs, client_models, client_optimizers, server_model, criterion, client_dls_list, test_dl, num):\n",
        "    train_loss, train_acc, test_loss, test_acc, anomalies = [], [], [], [], []\n",
        "    attack=None\n",
        "    train_attack=[]\n",
        "\n",
        "    # training phase 1\n",
        "    for epoch in range(drift_round*2):\n",
        "        if epoch % drift_round == 0:\n",
        "            print(\"---------------drift occur-----------------\")\n",
        "            # Calculate the index for the current phase\n",
        "            phase_index = epoch // drift_round\n",
        "            client_dls = client_dls_list[phase_index-1]\n",
        "\n",
        "        # local train\n",
        "        trainl, traina, historical_grads, current_grad, attacked_clients = train_clients(client_models, client_optimizers, server_model, criterion, client_dls, num, attack) # ATTACK\n",
        "        train_loss.append(trainl)\n",
        "        train_acc.append(traina)\n",
        "\n",
        "        # fedavg\n",
        "        loss_difference_1 = 1\n",
        "        loss_difference_2 = 1\n",
        "        if len(train_loss) >= 3:\n",
        "            loss_difference_1 = train_loss[-2] - train_loss[-1]\n",
        "            loss_difference_2 = train_loss[-3] - train_loss[-2]\n",
        "        fedavg(client_models, server_model, historical_grads, current_grad, loss_difference_1, loss_difference_2)\n",
        "\n",
        "        testl , testa = test(server_model, test_dl, criterion)\n",
        "\n",
        "        train_attack.append(attack)\n",
        "        test_loss.append(testl)\n",
        "        test_acc.append(testa)\n",
        "        # print(f'Epoch {epoch} - train_loss : {trainl :.4f}, train_acc : {traina:.4f}, test_loss : {testl:.4f}, test_acc : {testa:0.4f}')\n",
        "\n",
        "    history = {\n",
        "               'train_loss' : train_loss,\n",
        "               'train_acc' : train_acc,\n",
        "               'test_loss' : test_loss,\n",
        "               'test_acc' : test_acc,\n",
        "               'train_attacked_history' : train_attack\n",
        "               }\n",
        "    return history\n"
      ],
      "metadata": {
        "id": "jaqBuAGemh62"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = 20\n",
        "epoch = 80\n",
        "\n",
        "client_dls_list, client_models, client_optimizers = iid_clients(train_ds, n, epoch)\n",
        "server_model = MnistNet()"
      ],
      "metadata": {
        "id": "dCk7GCi_ROOJ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iid_fedavg_history = fit_fedavg(epochs, client_models, client_optimizers, server_model, criterion, client_dls_list, test_dl, n)"
      ],
      "metadata": {
        "id": "5mpPkBZdf4Ij",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f253d50c-cbad-4c91-b15a-4aac84bb4117"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------drift occur-----------------\n",
            "cluster results of historical [0 1 2 3 4 5 6 7 8]\n",
            "cluster results of current [0 1 2 3 4 5 6 7 8]\n",
            "cluster results of historical [0 1 2 3 4 5 6 7 8]\n",
            "cluster results of current [0 1 2 3 4 5 6 7 8]\n"
          ]
        }
      ]
    }
  ]
}